{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ae57ca",
   "metadata": {},
   "source": [
    "# Process S3 Parquet Files - Scratch Environment\n",
    "2023.01.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1788d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DateType, DoubleType\n",
    "from pyspark.sql.functions import col, lit, array, explode, arrays_zip, posexplode, \\\n",
    "    sum as sum_, max as max_, min as min_, year, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29aeeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_PROFILE\"] = \"service_wp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f99e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_wp\r\n"
     ]
    }
   ],
   "source": [
    "!echo $AWS_PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fcab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set spark configuration\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('process-weather-stations') \\\n",
    "    .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.2') \\\n",
    "    .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.profile.ProfileCredentialsProvider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5eabcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/01 15:37:49 WARN Utils: Your hostname, hp-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.167 instead (on interface wlp1s0)\n",
      "23/02/01 15:37:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/kishan/spark/spark-3.3.1-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kishan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kishan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-535359e6-7bd5-4b4b-af19-5931e81e89db;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 353ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-535359e6-7bd5-4b4b-af19-5931e81e89db\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/01 15:37:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# instantiate Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c74071",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_schema = StructType([StructField('STATION', StringType(), True),\n",
    " StructField('NAME', StringType(), True),\n",
    " StructField('LATITUDE', DoubleType(), True),\n",
    " StructField('LONGITUDE', DoubleType(), True),\n",
    " StructField('ELEVATION', DoubleType(), True),\n",
    " StructField('DATE', DateType(), True),\n",
    " StructField('HR00Val', LongType(), True),\n",
    " StructField('HR00MF', StringType(), True),\n",
    " StructField('HR00QF', StringType(), True),\n",
    " StructField('HR00S1', StringType(), True),\n",
    " StructField('HR00S2', StringType(), True),\n",
    " StructField('HR01Val', LongType(), True),\n",
    " StructField('HR01MF', StringType(), True),\n",
    " StructField('HR01QF', StringType(), True),\n",
    " StructField('HR01S1', StringType(), True),\n",
    " StructField('HR01S2', StringType(), True),\n",
    " StructField('HR02Val', LongType(), True),\n",
    " StructField('HR02MF', StringType(), True),\n",
    " StructField('HR02QF', StringType(), True),\n",
    " StructField('HR02S1', StringType(), True),\n",
    " StructField('HR02S2', StringType(), True),\n",
    " StructField('HR03Val', LongType(), True),\n",
    " StructField('HR03MF', StringType(), True),\n",
    " StructField('HR03QF', StringType(), True),\n",
    " StructField('HR03S1', StringType(), True),\n",
    " StructField('HR03S2', StringType(), True),\n",
    " StructField('HR04Val', LongType(), True),\n",
    " StructField('HR04MF', StringType(), True),\n",
    " StructField('HR04QF', StringType(), True),\n",
    " StructField('HR04S1', StringType(), True),\n",
    " StructField('HR04S2', StringType(), True),\n",
    " StructField('HR05Val', LongType(), True),\n",
    " StructField('HR05MF', StringType(), True),\n",
    " StructField('HR05QF', StringType(), True),\n",
    " StructField('HR05S1', StringType(), True),\n",
    " StructField('HR05S2', StringType(), True),\n",
    " StructField('HR06Val', LongType(), True),\n",
    " StructField('HR06MF', StringType(), True),\n",
    " StructField('HR06QF', StringType(), True),\n",
    " StructField('HR06S1', StringType(), True),\n",
    " StructField('HR06S2', StringType(), True),\n",
    " StructField('HR07Val', LongType(), True),\n",
    " StructField('HR07MF', StringType(), True),\n",
    " StructField('HR07QF', StringType(), True),\n",
    " StructField('HR07S1', StringType(), True),\n",
    " StructField('HR07S2', StringType(), True),\n",
    " StructField('HR08Val', LongType(), True),\n",
    " StructField('HR08MF', StringType(), True),\n",
    " StructField('HR08QF', StringType(), True),\n",
    " StructField('HR08S1', StringType(), True),\n",
    " StructField('HR08S2', StringType(), True),\n",
    " StructField('HR09Val', LongType(), True),\n",
    " StructField('HR09MF', StringType(), True),\n",
    " StructField('HR09QF', StringType(), True),\n",
    " StructField('HR09S1', StringType(), True),\n",
    " StructField('HR09S2', StringType(), True),\n",
    " StructField('HR10Val', LongType(), True),\n",
    " StructField('HR10MF', StringType(), True),\n",
    " StructField('HR10QF', StringType(), True),\n",
    " StructField('HR10S1', StringType(), True),\n",
    " StructField('HR10S2', StringType(), True),\n",
    " StructField('HR11Val', LongType(), True),\n",
    " StructField('HR11MF', StringType(), True),\n",
    " StructField('HR11QF', StringType(), True),\n",
    " StructField('HR11S1', StringType(), True),\n",
    " StructField('HR11S2', StringType(), True),\n",
    " StructField('HR12Val', LongType(), True),\n",
    " StructField('HR12MF', StringType(), True),\n",
    " StructField('HR12QF', StringType(), True),\n",
    " StructField('HR12S1', StringType(), True),\n",
    " StructField('HR12S2', StringType(), True),\n",
    " StructField('HR13Val', LongType(), True),\n",
    " StructField('HR13MF', StringType(), True),\n",
    " StructField('HR13QF', StringType(), True),\n",
    " StructField('HR13S1', StringType(), True),\n",
    " StructField('HR13S2', StringType(), True),\n",
    " StructField('HR14Val', LongType(), True),\n",
    " StructField('HR14MF', StringType(), True),\n",
    " StructField('HR14QF', StringType(), True),\n",
    " StructField('HR14S1', StringType(), True),\n",
    " StructField('HR14S2', StringType(), True),\n",
    " StructField('HR15Val', LongType(), True),\n",
    " StructField('HR15MF', StringType(), True),\n",
    " StructField('HR15QF', StringType(), True),\n",
    " StructField('HR15S1', StringType(), True),\n",
    " StructField('HR15S2', StringType(), True),\n",
    " StructField('HR16Val', LongType(), True),\n",
    " StructField('HR16MF', StringType(), True),\n",
    " StructField('HR16QF', StringType(), True),\n",
    " StructField('HR16S1', StringType(), True),\n",
    " StructField('HR16S2', StringType(), True),\n",
    " StructField('HR17Val', LongType(), True),\n",
    " StructField('HR17MF', StringType(), True),\n",
    " StructField('HR17QF', StringType(), True),\n",
    " StructField('HR17S1', StringType(), True),\n",
    " StructField('HR17S2', StringType(), True),\n",
    " StructField('HR18Val', LongType(), True),\n",
    " StructField('HR18MF', StringType(), True),\n",
    " StructField('HR18QF', StringType(), True),\n",
    " StructField('HR18S1', StringType(), True),\n",
    " StructField('HR18S2', StringType(), True),\n",
    " StructField('HR19Val', LongType(), True),\n",
    " StructField('HR19MF', StringType(), True),\n",
    " StructField('HR19QF', StringType(), True),\n",
    " StructField('HR19S1', StringType(), True),\n",
    " StructField('HR19S2', StringType(), True),\n",
    " StructField('HR20Val', LongType(), True),\n",
    " StructField('HR20MF', StringType(), True),\n",
    " StructField('HR20QF', StringType(), True),\n",
    " StructField('HR20S1', StringType(), True),\n",
    " StructField('HR20S2', StringType(), True),\n",
    " StructField('HR21Val', LongType(), True),\n",
    " StructField('HR21MF', StringType(), True),\n",
    " StructField('HR21QF', StringType(), True),\n",
    " StructField('HR21S1', StringType(), True),\n",
    " StructField('HR21S2', StringType(), True),\n",
    " StructField('HR22Val', LongType(), True),\n",
    " StructField('HR22MF', StringType(), True),\n",
    " StructField('HR22QF', StringType(), True),\n",
    " StructField('HR22S1', StringType(), True),\n",
    " StructField('HR22S2', StringType(), True),\n",
    " StructField('HR23Val', LongType(), True),\n",
    " StructField('HR23MF', StringType(), True),\n",
    " StructField('HR23QF', StringType(), True),\n",
    " StructField('HR23S1', StringType(), True),\n",
    " StructField('HR23S2', StringType(), True),\n",
    " StructField('DlySum', LongType(), True),\n",
    " StructField('DlySumMF', StringType(), True),\n",
    " StructField('DlySumQF', StringType(), True),\n",
    " StructField('DlySumS1', StringType(), True),\n",
    " StructField('DlySumS2', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5243c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.schema(station_schema).parquet('s3a://weather-data-kpde/raw/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f4413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23259498"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7056c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a640fdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 19:42:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df.coalesce(1).write.mode('overwrite').parquet('s3a://weather-data-kpde/out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ed5b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate hourly columns\n",
    "def generate_cols_as_array(col_name):\n",
    "    return array([col(f'HR{i:02}{col_name}') for i in range(24)]).alias(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61cae246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack hourly columns\n",
    "col_types = ['Val', 'MF', 'QF', 'S1', 'S2']\n",
    "\n",
    "df_t = (df.select('STATION', 'DATE', arrays_zip(*[generate_cols_as_array(c) for c in col_types]\n",
    "                                          ).alias('zip'))  # generate zipped arrays for each hour\n",
    " .select('*', posexplode('zip').alias('hr', 'exp'))  # explode zipped arrays\n",
    " .select('*', *[col('exp')[c].alias(c) for c in col_types],  # extract array\n",
    "        )\n",
    " .select('STATION', 'DATE', 'hr', *col_types)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f098d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace -9999 values and ' ' strings with null\n",
    "df_t = (df_t.replace({-9999: None}, subset=['Val'])\n",
    "     .replace({' ': None}, subset=['MF', 'QF', 'S1', 'S2'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b216e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_t.withColumn('year', year('DATE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beef778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f8c60d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_t.write.mode('overwrite').parquet('s3a://weather-data-kpde/out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c72257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---+---+---+----+---+----+----+\n",
      "|    STATION|      DATE| hr|Val| MF|  QF| S1|  S2|year|\n",
      "+-----------+----------+---+---+---+----+---+----+----+\n",
      "|USC00355969|1948-07-01|  0|  0|  Z|null|  4|null|1948|\n",
      "|USC00355969|1948-07-01|  1|  0|  Z|null|  4|null|1948|\n",
      "|USC00355969|1948-07-01|  2|  0|  Z|null|  4|null|1948|\n",
      "|USC00355969|1948-07-01|  3|  0|  Z|null|  4|null|1948|\n",
      "|USC00355969|1948-07-01|  4|  0|  Z|null|  4|null|1948|\n",
      "+-----------+----------+---+---+---+----+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_t.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403e24e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/01 16:12:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                (8 + 4) / 45]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kishan/spark/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/kishan/spark/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7784/2327697907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://weather-data-kpde/out/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     def text(\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# repartitioning is very expensive\n",
    "# df_t.write.mode('overwrite').partitionBy('year').parquet('s3a://weather-data-kpde/out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b34a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee55c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27799"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f333db",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177dc023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
